# Chapter 12  Networked programs

## 12.1 Hypertext Transfer Protocol - HTTP

* **Transport control protocol (TCP)**

   * Built on top of IP (Internet Protocol)
   * Assumes IP might lose some data-stores and retransmits data if it seems to be lost
   * Handles "flow control" using a transmit window
   * Provides a nice reliable pipe 

* **TCP Connections / socket** : built-in support in Python

   * like a file
   
   * A single socket provides a two-way connection between two programs. You can read from and write to the same socket.

* **TCP Port Numbers**

   * A port is an application-specific or process-specific software communications endpoint
   * It allows multiple networked applications to coexist on the same server
   * There is a list of well-known TCP port numbers
 
* **Common TCP Ports**

   * Telnet (23) - Login
   * SSH (22) - Secure Login
   * HTTP (80)
   * HTTPS (443) -Secure
   * SMTP (25) - Mail
   * IMAP (143/220/993) - Mail
   * POP (109/110) - Mail
   * DNS (53) - Domain Name
   * FTP (21) - File Transfer

* **Sockets in Python**

   * Python has built-in support for TCP Sockets

```python
>>> import socket
>>> mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
>>> mysock.connect(('data.pr4e.org', 80))   # Host(data.pr4e.org); Port(80)
```

* **HTTP - Hypertext Transfer Protocol**

   * The dominant Application Layer Protocol on the Internet
   * Invented for the Web - to Retrieve HTML, Images, Documents, etc.
   * Extended to be data in addition to documents - RSS, Web Services, etc. 
      * Basic Concept - Make a Connection - Request a document - Retrieve the Document - Close the Connection
      
> http://www.dr-chuck.com/pagel.htm   
> * protocol: http://     
> * host: www.dr-chuck.com     
> * document: pagel.htm     

## 12.2 The world's simplest web browser

* `GET` request : To request a document from a web server, we make a connection to the www.pr4e.org server on port 80, and then send a line of the form 
   * `GET http://data.pr4e.org/romeo.txt HTTP/1.0`
* Your program: socket -> connect -> send -> recv
```python
import socket

mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('data.pr4e.org', 80))
cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\r\n\r\n'.encode()
# \r\n signifies an EOL (end of line)
# \r\n\r\n signifies nothing between two EOL sequences. That is the equivalent of a blank line.
mysock.send(cmd)

while True:
    data = mysock.recv(512)
    if len(data) < 1:
        break
    print(data.decode(), end='')
mysock.close()

$ python3 http.py
HTTP/1.1 200 OK
Date: Wed, 17 Mar 2021 20:41:54 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
```

* **ASCII - American Standard Code for Information Interchange**

   * Each character is represented by a number between 0 and 256 stored in 8 bits of memory
   * We refer to "8 bits of memory as a "byte" of memory - (i.e. my disk drive contains 3 Terabytes of memory)
   * The `ord()` function tells us the numeric value of a simple ASCII character

```python
>>> ord('H')
72
>>> ord('e')
101
>>> print(ord('\n'))
10
```

* **Multi-Byte Characters** : To represent the wide range of characters computers must handle we represent characters with more than one byte

   * UTF-16 - Fixed length - Two bytes
   * UTF-32 - Fixed length - Four bytes
   * UTF-8 - 1-4 bytes
      * Upwards compatible with ASCII
      * Automatic detection between ASCII and UTF-8
      * UTF-8 is recommended practice for encoding data to be exchaned between systems

* **Python3 and Unicode**

   * In Python3, all strings internally are UNICODE
   * Working with string variables in python programs and reading data from files usually "just works"
   * When we talk to a network resource using sockets or talk to a database we have to encode and decode data (usually to UTF-8)
   
```python
>>> x = b'abc'
>>> type(x)
<class 'bytes'>
>>> x = '이광춘'
>>> type(x)
<class 'str'>
>>> x = u'이광춘'
>>> type(x)
<class 'str'>
```

   *  encode() and decode() methods convert strings into bytes objects and back again
      * a bytes object.encode() and b'' are equivalent: a variable should be stored as a bytes object
```
>>> b'Hello world'
b'Hello world'
>>> 'Hello world'.encode()
b'Hello world'
```
> * `encode()`: String Unicode --> Bytes UTF-8
> * `decode()`: Bytes UTF-8 --> String Unicode
> * `send()`: Bytes UTF-8 --> Socket
> * `recv()`: Socket --> Bytes UTF-8

* **Using urllib in Python**

   * Since HTTP is so common, we have a library that does all the socket work for us and makes web pages look like a file.

```python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')
for line in fhand:
    print(line.decode().strip())

$ python3 urllib.py
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
```

```python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')

counts = dict()
for line in fhand:
    words = line.decode().split()
    for word in words:
        counts[word] = counts.get(word, 0) + 1
print(counts)

$ python3 urlwords.py
{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}
```

   * Reading Web Pages
   

## 12.3 Retrieving an image over HTTP

```python
import socket
import time

HOST = 'data.pr4e.org'
PORT = 80
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect((HOST, PORT))
mysock.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\r\n\r\n')
count = 0
picture = b""

while True:
    data = mysock.recv(5120)
    if len(data) < 1 : break
    # time.sleep(0.25)
    count = count + len(data)
    print(len(data), count)
    picture = picture + data

mysock.close()

# look for the end of the header (2 CRLF)
pos = picture.find(b"\r\n\r\n")
print('Header length', pos)
print(picture[:pos].decode())

# skip past the header and save the picture data
picture = picture[pos+4:]
fhand = open("stuff.jpg", "wb")
fhand.write(picture)
fhand.close()

$ python3 image.py
4344 4344
4344 8688
2896 11584
1448 13032
...
2896 222992
2896 225888
1448 227336
1448 228784
1824 230608
Header length 394
HTTP/1.1 200 OK
Date: Fri, 19 Mar 2021 03:08:04 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
```

## 12.4 Retrieving web pages with urllib

```python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')
for line in fhand:
    print(line.decode().strip())

$ python3 urllib2.py 
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
```

## 12.5 Reading binary files using urllib

```python
import urllib.request, urllib.parse, urllib.error

img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg').read()
fhand = open('cover3.jpg', 'wb')
fhand.write(img)
fhand.close()
```

```python
import urllib.request, urllib.parse, urllib.error

img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg') 
fhand = open('cover3.jpg', 'wb')
size = 0
while True:
	info = img.read(100000) 
	if len(info) < 1: break 
	size = size + len(info) 
	fhand.write(info)
print(size, 'characters copied.')
fhand.close()

$ python curl2.py
230210 characters copied.
```

## 12.6 Parsing HTML and scraping the web

* **What is Web Scraping**

   * When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages
   * Search engines scrape web pages - we call this “spidering the web” or “web crawling”

* **Why scrape?**

   * Pull data - particularly social data - who links to who?
   * Get your own data back out of some system that has no “export capability”
   * Monitor a site for new information
   * Spider the web to make a database for a search engine

* **Scraping Web Pages**

   * There is some controversy about web page scraping and some sites are a bit snippy about it.
   * Republishing copyrighted information is not allowed
   * Violating terms of service is not allowed

## 12.7 Parsing HTML using regular expressions

```python
# Search for link values within URL input
import urllib.request, urllib.parse, urllib.error
import re
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
html = urllib.request.urlopen(url, context=ctx).read() links = re.findall(b'href="(http[s]?://.*?)"', html) 
for link in links:
    print(link.decode())

$ python3
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
```

## 12.8 Parsing HTML using BeautifulSoup

* BeautifulSoup : free software library from www.crummy.com

* BeautifulSoup Installation

> urllinks.py

```python
# To run this, you can install BeautifulSoup
# https://pypi.python.org/pypi/beautifulsoup4

# Or download the file
# http://www.py4e.com/code3/bs4.zip
# and unzip it in the same directory as this file

import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
```

```python
import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup

url = input('Enter - ')
html = urllib.request.urlopen(url).read()
soup = BeautifulSoup(html, 'html.parser')

# Retrieve all of the anchor tags
tags = soup('a')
for tag in tags:
    print(tag.get('href', None))

$ python3 urllinks.py 
Enter - http://www.dr-chuck.com/page1.htm
http://www.dr-chuck.com/page2.htm
```
## 12.9 Bonus section for Unix / Linux users

## 12.10 Glossary

BeautifulSoup A Python library for parsing HTML documents and extracting data from HTML documents that compensates for most of the imperfections in the HTML that browsers generally ignore. You can download the Beauti- fulSoup code from www.crummy.com.

port A number that generally indicates which application you are contacting when you make a socket connection to a server. As an example, web traffic usually uses port 80 while email traffic uses port 25.

scrape When a program pretends to be a web browser and retrieves a web page, then looks at the web page content. Often programs are following the links in one page to find the next page so they can traverse a network of pages or a social network.

socket A network connection between two applications where the applications can send and receive data in either direction.

spider The act of a web search engine retrieving a page and then all the pages linked from a page and so on until they have nearly all of the pages on the Internet which they use to build their search index.

## 12.11 Exercises

Exercise 1: Change the socket program socket1.py to prompt the user for the URL so it can read any web page. You can use split('/') to break the URL into its component parts so you can extract the host name for the socket connect call. Add error checking using try and except to handle the condition where the user enters an improperly formatted or non-existent URL.


Exercise 2: Change your socket program so that it counts the number of characters it has received and stops displaying any text after it has shown 3000 characters. The program should retrieve the entire docu- ment and count the total number of 
characters and display the count of the number of characters at the end of the document.


Exercise 3: Use urllib to replicate the previous exercise of (1) retrieving the document from a URL, (2) displaying up to 3000 characters, and (3) counting the overall number of characters in the document. Don’t worry about the headers for this exercise, simply show the first 3000 characters of the document contents.


Exercise 4: Change the urllinks.py program to extract and count para- graph (p) tags from the retrieved HTML document and display the count of the paragraphs as the output of your program. Do not display the paragraph text, only count them. Test your program on several small web pages as well as some larger web pages.


Exercise 5: (Advanced) Change the socket program so that it only shows data after the headers and a blank line have been received. Remember that recv receives characters (newlines and all), not lines.
